{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import torch,torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "from model import Generator, iterate_minibatches, compute_loss, train\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import time\n",
    "import torch.utils.data\n",
    "from tqdm import tqdm\n",
    "from generate import write_file, generate\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_models import LstmDiscriminator, FeaturedLstmDiscriminator, LstmBaseline\n",
    "from scores import EB, UPC, QN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g_loss(p_fake,sound,in_probs,p_baseline, eps = 1e-8):\n",
    "        \n",
    "    probs = sound[:,:,:,:2]*in_probs[:,:,:,:2]\\\n",
    "            +(1-sound[:,:,:,:2])*(1-in_probs[:,:,:,:2])\n",
    "        \n",
    "    return -((probs+eps).log().sum(dim =-1).sum(dim =-1).sum(dim =-1)*(p_fake-p_baseline)).mean(), probs\n",
    "\n",
    "def g_loss_monte_carlo(p_fakes,sounds,in_probs,p_baseline, eps = 1e-8):\n",
    "    \n",
    "    loss = 0\n",
    "    for sound, p_fake in zip(sounds, p_fakes):\n",
    "        probs = sound[:,:,:,:2]*in_probs[:,:,:,:2]\\\n",
    "                +(1-sound[:,:,:,:2])*(1-in_probs[:,:,:,:2])\n",
    "        loss += -((probs+eps).log().sum(dim =-1).sum(dim =-1).sum(dim =-1)*(p_fake-p_baseline)).mean()\n",
    "        \n",
    "    return loss/5, probs\n",
    "\n",
    "def d_loss(p_fake, p_true,eps = 1e-8):\n",
    "    return -(1-p_fake+eps).log().mean()-(p_true+eps).log().mean()\n",
    "    \n",
    "def bl_loss(bl_pred,real_reward):\n",
    "    return (bl_pred-real_reward).pow(2).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data\n",
    "\n",
    "def sample_sound(data_gen):\n",
    "    size = data_gen.size()\n",
    "    rand = torch.rand(*size).cuda()\n",
    "    sample = (rand<data_gen).type(torch.FloatTensor).cuda()\n",
    "    sample[:,:,:,2] = sample[:,:,:,0]\n",
    "    return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "OCTAVE_NUM = 4\n",
    "NOTE_NUM = 12\n",
    "TIME_SCALE = 128\n",
    "\n",
    "from dataset import load_all\n",
    "from constants import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5727, 128, 48, 3) (5727, 128, 48, 3)\n"
     ]
    }
   ],
   "source": [
    "styles= [['data/Bach']]\n",
    "X_tr, y_tr = load_all(styles, BATCH_SIZE, SEQ_LEN)\n",
    "print(X_tr.shape, y_tr.shape)\n",
    "\n",
    "X_loader = torch.utils.data.DataLoader(\\\n",
    "            torch.utils.data.TensorDataset(\\\n",
    "            *(torch.FloatTensor(X_tr),\n",
    "            torch.FloatTensor(y_tr))),\\\n",
    "            batch_size=8,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_discriminator(num_epochs = 3,d_lr = 0.001, verbose = True):\n",
    "    \n",
    "    X_loader = torch.utils.data.DataLoader(\\\n",
    "            torch.utils.data.TensorDataset(\\\n",
    "            *(torch.FloatTensor(X_tr),\n",
    "            torch.FloatTensor(y_tr))),\\\n",
    "            batch_size=8,shuffle=True)\n",
    "\n",
    "    d_optimizer = torch.optim.Adam(discriminator.parameters(), lr=d_lr, betas=(0.5, 0.999)) #, weight_decay = 1, betas=(0.5, 0.999))  \n",
    "    \n",
    "    history = {'d_losses':[], 'prop_machine':[],\n",
    "               'prob_human':[], 'prob_noise':[], 'color':[]}\n",
    "\n",
    "    generator.eval()\n",
    "    generator.note_ax.to_train=True\n",
    "    generator.note_ax.apply_T=False\n",
    "        \n",
    "    for epoch in range(num_epochs):        \n",
    "        start_time = time.time()\n",
    "        for i, [x_batch,ch_batch] in tqdm(enumerate(X_loader)):\n",
    "            try:\n",
    "                x_batch = x_batch.cuda()\n",
    "                ch_batch = ch_batch.cuda()\n",
    "                \n",
    "                #x_batch[:,:,:,2] = x_batch[:,:,:,0] #1 #x_batch[:,:,:,0]\n",
    "                ch_batch[:,:,:,2] = ch_batch[:,:,:,0] # 1 #ch_batch[:,:,:,0]\n",
    "\n",
    "                # Optimize D  \n",
    "                # rand_like ones_like            \n",
    "                noise_example = sample_sound(torch.rand_like(x_batch).cuda()), x_batch\n",
    "                true_example = ch_batch , x_batch\n",
    "                p_true_example = discriminator(*true_example)\n",
    "                p_noise_example = discriminator(*noise_example)\n",
    "                loss = d_loss(p_noise_example, p_true_example)\n",
    "                d_optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                d_optimizer.step()            \n",
    "\n",
    "                # machine generation\n",
    "                data_gen, sound = generator(x_batch,ch_batch)\n",
    "#                 sound = sample_sound(data_gen).data          \n",
    "                false_example = sound, x_batch\n",
    "                true_example = ch_batch, x_batch\n",
    "                p_true_example = discriminator(*true_example)\n",
    "                p_false_example = discriminator(*false_example)\n",
    "                loss = d_loss(p_false_example, p_true_example)\n",
    "                \n",
    "                d_optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                d_optimizer.step()\n",
    "                \n",
    "                # history\n",
    "                history['d_losses'].append(loss.data.cpu().numpy())\n",
    "                history['prop_machine'].append(p_false_example.mean().data.cpu().numpy())\n",
    "                history['prob_human'].append(p_true_example.mean().data.cpu().numpy())\n",
    "                history['prob_noise'].append(p_noise_example.mean())\n",
    "                \n",
    "            except KeyboardInterrupt:\n",
    "                return history, True\n",
    "\n",
    "    #       # Visualize\n",
    "        if verbose:\n",
    "            display.clear_output(wait=True)\n",
    "            plt.figure(figsize=(16, 6))\n",
    "\n",
    "            # Then we print the results for this epoch:\n",
    "            print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "                epoch + 1, num_epochs, time.time() - start_time)) \n",
    "            print('mean probability of human generation: {}'.format(history['prob_human'][-1]))\n",
    "#             print('mean probability of noise generation: {}'.format(prob_noise[-1]))\n",
    "            print('mean probability of machine generation: {}'.format(history['prop_machine'][-1]))\n",
    "\n",
    "            plt.title(\"losses\")\n",
    "            plt.subplot(121)\n",
    "            plt.plot(history['prob_human'],label = \"probability of human generation\")\n",
    "            plt.plot(history['prop_machine'],label = \"probability of machine generation\")\n",
    "            plt.legend()\n",
    "            plt.subplot(122)\n",
    "            plt.plot(history['d_losses'],label = \"Discriminator loss\")\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "                \n",
    "            \n",
    "    return history, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# h= train_discriminator( num_epochs = 1, d_lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generator(num_epochs = 3,g_lr = 0.001, verbose=True, monte_carlo=True, \n",
    "                    entropy_loss = True, teacher_forcing = 2):\n",
    "    \n",
    "    g_optimizer = torch.optim.Adam(generator.parameters(),  lr=g_lr, betas=(0.5, 0.999))#, betas=(0.5, 0.999))\n",
    "    \n",
    "    history = {'g_losses':[],'prop_machine':[],\n",
    "               'prob_human':[],'prob_confidence':[],'prob_noise':[],\n",
    "                  'EB_true':[], 'UPC_true':[], 'QN_true':[], 'EB_false':[], 'UPC_false':[], 'QN_false':[]}\n",
    "    \n",
    "    generator.train()\n",
    "    generator.note_ax.to_train=True\n",
    "    generator.note_ax.apply_T=False\n",
    "    \n",
    "    X_loader = torch.utils.data.DataLoader(\\\n",
    "            torch.utils.data.TensorDataset(\\\n",
    "            *(torch.FloatTensor(X_tr),\n",
    "            torch.FloatTensor(y_tr))),\\\n",
    "            batch_size=3,shuffle=True)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        for i, [x_batch,ch_batch] in tqdm(enumerate(X_loader)):\n",
    "            \n",
    "            try:\n",
    "                \n",
    "                x_batch = x_batch.cuda()\n",
    "                ch_batch = ch_batch.cuda()\n",
    "                \n",
    "                #x_batch[:,:,:,2] = x_batch[:,:,:,0] #1 #x_batch[:,:,:,0]\n",
    "                ch_batch[:,:,:,2] = ch_batch[:,:,:,0] #1 #ch_batch[:,:,:,0]\n",
    "\n",
    "                # baseline\n",
    "                p_baseline = baseline(x_batch,ch_batch)\n",
    "\n",
    "                #generator\n",
    "                # monte carlo false example sampling\n",
    "                data_gen, sound = generator(x_batch,ch_batch)\n",
    "                if monte_carlo:\n",
    "                    p_false_examples = []\n",
    "                    sounds = []\n",
    "\n",
    "                    for _ in range(5):                  \n",
    "    #                     sound = sample_sound(data_gen).data                 \n",
    "                        false_example = sound.detach(), x_batch.detach()\n",
    "                        p_false_example = discriminator(*false_example)\n",
    "                        p_false_examples.append(p_false_example)\n",
    "                        sounds.append(sound)\n",
    "                else:\n",
    "    #                 sound = sample_sound(data_gen).data                 \n",
    "                    false_example = sound.detach(), x_batch.detach()\n",
    "                    p_false_example = discriminator(*false_example)\n",
    "\n",
    "\n",
    "                true_example = ch_batch.detach(), x_batch.detach()\n",
    "                p_true_example = discriminator(*true_example)\n",
    "\n",
    "                # Optimize G  \n",
    "                handle = np.random.randint(0,teacher_forcing-1)\n",
    "#                 handle = 0\n",
    "                if handle != 0:\n",
    "                    if monte_carlo:\n",
    "                        loss, confidence = g_loss_monte_carlo(p_false_examples,\n",
    "                                  sounds,data_gen,p_baseline)\n",
    "\n",
    "                    else:\n",
    "                        loss, confidence = g_loss(p_false_example,\n",
    "                                  sound,data_gen,p_baseline)\n",
    "\n",
    "                else:\n",
    "                    # teaher forcing\n",
    "                    loss, confidence = g_loss(1,\n",
    "                                  ch_batch,data_gen,p_baseline)\n",
    "\n",
    "                if entropy_loss:\n",
    "                    loss+=compute_loss(data_gen, ch_batch)   \n",
    "                    \n",
    "                g_optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                g_optimizer.step()\n",
    "                \n",
    "            except KeyboardInterrupt:\n",
    "                return history, True\n",
    "        \n",
    "            # history\n",
    "            history['g_losses'].append(loss.data.cpu().numpy())\n",
    "            history['prob_human'].append(p_true_example.mean().data.cpu().numpy())\n",
    "            history['prop_machine'].append(p_false_example.mean().data.cpu().numpy())\n",
    "            history['prob_confidence'].append(confidence.mean().data.cpu().numpy())\n",
    "            history['EB_true'].append(EB(ch_batch).mean().data.cpu().numpy())\n",
    "            history['UPC_true'].append(UPC(ch_batch).mean().data.cpu().numpy())\n",
    "            history['QN_true'].append(QN(ch_batch).mean().data.cpu().numpy())\n",
    "            history['EB_false'].append(EB(sound).mean().data.cpu().numpy())\n",
    "            history['UPC_false'].append(UPC(sound).mean().data.cpu().numpy())\n",
    "            history['QN_false'].append(QN(sound).mean().data.cpu().numpy())\n",
    "\n",
    "        # Visualize\n",
    "        if verbose:\n",
    "            display.clear_output(wait=True)\n",
    "            plt.figure(figsize=(16, 6))\n",
    "\n",
    "            # Then we print the results for this epoch:\n",
    "            print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "                epoch + 1, num_epochs, time.time() - start_time)) \n",
    "            print('mean probability of human generation: {}'.format(history['prob_human'][-1]))\n",
    "            print('mean probability of machine generation: {}'.format(history['prop_machine'][-1]))\n",
    "    #         print('mean probability of noise generation: {}'.format(prob_noise[-1]))\n",
    "            print('mean probability of machine generation confidence: {}'.format(history['prob_confidence'][-1]))\n",
    "\n",
    "\n",
    "            plt.title(\"losses\")\n",
    "            plt.subplot(231)\n",
    "            plt.plot(history['prob_human'], label = \"Human probability\")\n",
    "            plt.plot(history['prop_machine'], label = \"Machine probability\")\n",
    "            plt.legend()\n",
    "            plt.subplot(232)\n",
    "            plt.plot(history['g_losses'], label = \"Generator loss\")\n",
    "            plt.legend()\n",
    "            plt.subplot(233)\n",
    "            plt.plot(history['prob_confidence'],label = \"Machine confidence\")\n",
    "            plt.legend()\n",
    "            plt.subplot(234)\n",
    "            plt.plot(history['EB_true'],label = \"EB_true\")\n",
    "            plt.plot(history['EB_false'],label = \"EB_false\")\n",
    "            plt.legend()\n",
    "            plt.subplot(235)\n",
    "            plt.plot(history['UPC_true'],label = \"UPC_true\")\n",
    "            plt.plot(history['UPC_false'],label = \"UPC_false\")\n",
    "            plt.legend()\n",
    "            plt.subplot(236)\n",
    "            plt.plot(history['QN_true'],label = \"QN_true\")\n",
    "            plt.plot(history['QN_false'],label = \"QN_false\")\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "  \n",
    "    return history, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_generator(num_epochs = 100, g_lr=1e-3, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_baseline(num_epochs = 3, b_lr = 0.001, verbose=True):\n",
    "    \n",
    "    bl_optimizer = torch.optim.Adam(baseline.parameters(),  lr=b_lr)\n",
    "    \n",
    "    bl_losses = []\n",
    "    prob_baseline = []     \n",
    "    generator.eval()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        for i, [x_batch,ch_batch] in tqdm(enumerate(X_loader)):\n",
    "            \n",
    "            x_batch = x_batch.cuda()\n",
    "            ch_batch = ch_batch.cuda()\n",
    "            \n",
    "            ch_batch[:,:,:,2] = ch_batch[:,:,:,0]\n",
    "            \n",
    "            # Optimize BL\n",
    "            data_gen, sound = generator(x_batch,ch_batch)\n",
    "#             sound = sample_sound(data_gen).data\n",
    "            false_example = sound.detach(), x_batch.detach()\n",
    "            p_baseline = baseline(x_batch,ch_batch)\n",
    "            p_false_example = discriminator(*false_example)\n",
    "            loss = bl_loss(p_baseline, p_false_example)\n",
    "            \n",
    "            bl_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            bl_optimizer.step()\n",
    "            \n",
    "            # history\n",
    "            bl_losses.append(loss.data.cpu().numpy())\n",
    "            prob_baseline.append(p_baseline.mean().data.cpu().numpy())\n",
    "\n",
    "            \n",
    "    return np.array(prob_baseline),np.array(bl_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discriminator = FeaturedLstmDiscriminator(hidden_size=1000,last_dim=78*2).cuda()\n",
    "# generator = Generator().cuda()\n",
    "# generator.load_state_dict(torch.load(os.path.join(OUT_DIR, 'model_canonical_attention')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# prob_human, prop_machine, d_losses = train_discriminator( num_epochs = 40, d_lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(discriminator.state_dict(), os.path.join(OUT_DIR, 'discriminator'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator.load_state_dict(torch.load(os.path.join(OUT_DIR, 'model_canonical')))\n",
    "# baseline = LSTM_baseline(hidden_size=10).cuda()\n",
    "# generator,prob_human, prop_machine, prob_confidence, g_losses= train_generator(generator, discriminator, baseline, X_loader, num_epochs = 30, g_lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '/data/i.anokhin/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    \n",
    "    plt.figure(figsize=(16, 6))\n",
    "    plt.title(\"losses\")\n",
    "    plt.subplot(331)\n",
    "    plt.plot(history['g_losses'],label = \"Generator loss\")\n",
    "    plt.legend()\n",
    "    plt.subplot(332)\n",
    "    plt.plot(history['d_losses'],label = \"Discriminator loss\")\n",
    "    plt.legend(loc=2)\n",
    "    plt.subplot(333)\n",
    "    plt.plot(history['bl_losses'],label = \"Baseline loss\")\n",
    "    plt.legend()\n",
    "    plt.subplot(334)\n",
    "    g_machine = np.asarray(history['prop_machine'].copy())\n",
    "    d_machine = np.asarray(history['prop_machine'].copy())\n",
    "    g_machine[np.asarray(history['color']) == 1] = np.nan\n",
    "    d_machine[np.asarray(history['color']) == 0] = np.nan\n",
    "    plt.plot(g_machine, 'o', label = \"Machine P - generator\", color = 'blue' )\n",
    "    plt.plot(d_machine, 'o', label = \"Machine P - discriminator\", color = 'orange' )\n",
    "    plt.plot(history['prob_human'], 'o', label = \"Human P\", color = 'green')\n",
    "    plt.legend(loc=2)\n",
    "    plt.subplot(335)\n",
    "    plt.plot(history['prob_confidence'],label = \"Machine confidence\")\n",
    "    plt.legend()\n",
    "    plt.subplot(336)\n",
    "    plt.plot(history['prop_baseline'],label = \"Baseline probabilyty\")\n",
    "    plt.legend()\n",
    "    plt.subplot(337)\n",
    "    plt.plot(history['EB_true'],label = \"EB_true\")\n",
    "    plt.plot(history['EB_false'],label = \"EB_false\")\n",
    "    plt.legend()\n",
    "    plt.subplot(338)\n",
    "    plt.plot(history['UPC_true'],label = \"UPC_true\")\n",
    "    plt.plot(history['UPC_false'],label = \"UPC_false\")\n",
    "    plt.legend()\n",
    "    plt.subplot(339)\n",
    "    plt.plot(history['QN_true'],label = \"QN_true\")\n",
    "    plt.plot(history['QN_false'],label = \"QN_false\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def train_GAN(g_lr = 0.001, d_lr = 0.001, bl_lr = 0.001, history = None, criterion=True, \n",
    "              verbose=True, dump_history=True):\n",
    "    \n",
    "    if not history:     \n",
    "        history = {'d_losses':[],'g_losses':[],'bl_losses':[],'prop_machine':[],\n",
    "               'prob_human':[],'prob_confidence':[1,],'prop_baseline':[], 'prob_noise':[], 'color':[],\n",
    "                  'EB_true':[], 'UPC_true':[], 'QN_true':[], 'EB_false':[], 'UPC_false':[], 'QN_false':[]}\n",
    "    \n",
    "    epoch = 0\n",
    "    n_epochs = 5000\n",
    "    it_generator = 0\n",
    "    it_discriminator = 0\n",
    "\n",
    "    while epoch<n_epochs:\n",
    "        start_time = time.time()\n",
    "        epoch+=1\n",
    "        \n",
    "        try:  \n",
    "            \n",
    "            #if ((epoch % 1) ==0):\n",
    "            if dump_history:\n",
    "                    pickle.dump(history, open(PATH+'rl_history.p', \"wb\" ))\n",
    "\n",
    "              \n",
    "            if criterion:\n",
    "                print('discriminator trainnig')\n",
    "                if it_generator>0:\n",
    "                    write_file('output/rl_test_'+str(epoch), generate(generator, 32, to_train=True))\n",
    "                    torch.save(generator.state_dict(), PATH+'generator_'+str(epoch))\n",
    "                    \n",
    "                it_generator = 0\n",
    "                it_discriminator+=1\n",
    "                \n",
    "                generator.eval()\n",
    "                h, flag = train_discriminator(num_epochs = 1, d_lr=d_lr, verbose = False)\n",
    "                \n",
    "                for k in h.keys():\n",
    "                    history[k].extend(h[k])\n",
    "                history['color'].extend(np.ones(len(h['prob_human'])))\n",
    "                \n",
    "            else: \n",
    "                print('generator trainnig')\n",
    "                if it_discriminator>0:\n",
    "                    torch.save(discriminator.state_dict(), PATH+'discriminator_'+str(epoch))\n",
    "                    \n",
    "                it_discriminator = 0\n",
    "                \n",
    "                p_baseline, bl_l = train_baseline(num_epochs = 1,b_lr = bl_lr, verbose=False)\n",
    "                history['prop_baseline'].extend(p_baseline)\n",
    "                history['bl_losses'].extend(bl_l) \n",
    "\n",
    "                it_generator+=1\n",
    "                generator.train()\n",
    "                h, flag = train_generator(num_epochs = 3, g_lr=g_lr, verbose = False, \n",
    "                                    monte_carlo = True, teacher_forcing=2, entropy_loss = True)\n",
    "                \n",
    "                for k in h.keys():\n",
    "                    history[k].extend(h[k])\n",
    "                history['color'].extend(np.zeros(len(h['prob_human'])))\n",
    "                \n",
    "            p_human = np.array(history['prob_human'][-100:])\n",
    "            p_machine =  np.array(history['prop_machine'][-100:])\n",
    "            diff = np.maximum(p_human-p_machine,0).mean()\n",
    "            criterion = (diff<0.1) or (p_human.mean()<0.8 and diff<0.3)\n",
    "#             and it_generator>5\n",
    "#             or (p_human.mean()<0.9 and diff<0.3)\n",
    "            if flag:\n",
    "                return epoch, history\n",
    "                \n",
    "        except KeyboardInterrupt:\n",
    "            return epoch, history  \n",
    "                    \n",
    "        \n",
    "        if verbose:\n",
    "            # Visualize\n",
    "            display.clear_output(wait=True)\n",
    "            # Then we print the results for this epoch:\n",
    "            if history['color'][-1]==0:\n",
    "                tr = 'generator trainig'\n",
    "            else: tr = 'last trainig - discriminator '\n",
    "            print(\"Epoch {} of {} took {:.3f}s, format {}\".format(\n",
    "                epoch, n_epochs, time.time() - start_time, tr))\n",
    "            print('mean probability of machine generation: {}'.format(history['prop_machine'][-1]))\n",
    "            print('mean probability of human generation: {}'.format(history['prob_human'][-1]))\n",
    "        #         print('mean probability of noise generation: {}'.format(prob_noise[-1]))\n",
    "            print('mean probability of machine generation confidence: {}'.format(history['prob_confidence'][-1]))\n",
    "\n",
    "            plot_history(history)\n",
    "            \n",
    "    return epoch, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epoch = 5\n",
    "# write_file('output/rl_test_'+str(epoch), generate(generator, 32, to_train=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discriminator = FeaturedLstmDiscriminator(hidden_size=1000,last_dim=78*2).cuda()\n",
    "# generator = Generator().cuda()\n",
    "# baseline = LstmBaseline(hidden_size=40).cuda()\n",
    "\n",
    "# generator.load_state_dict(torch.load(os.path.join(OUT_DIR, 'model_test2'))) #model_test2 enerator_rl\n",
    "# discriminator.load_state_dict(torch.load(os.path.join(OUT_DIR, 'discriminator')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discriminator.load_state_dict(torch.load(os.path.join(OUT_DIR, 'discriminator')))\n",
    "# generator.note_ax.to_train=True\n",
    "# generator.note_ax.apply_T=False\n",
    "# generator.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# h= train_discriminator( num_epochs = 20, d_lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p_baseline, bl_l = train_baseline(num_epochs = 1,b_lr = 0.001, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator = Generator().cuda()\n",
    "# generator.load_state_dict(torch.load(os.path.join(OUT_DIR, 'model_canonical_attention')))\n",
    "# generator.train()\n",
    "# h  = train_generator(num_epochs = 1, g_lr=1e-3, verbose = True, monte_carlo = False, \n",
    "#                      teacher_forcing=3, entropy_loss = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discriminator = FeaturedLstmDiscriminator(hidden_size=1000,last_dim=78*2).cuda()\n",
    "# generator = Generator().cuda()\n",
    "# baseline = LstmBaseline(hidden_size=40).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(generator.state_dict(), '/data/i.anokhin/generator')\n",
    "# torch.save(discriminator.state_dict(), '/data/i.anokhin/discriminator')\n",
    "# torch.save(baseline.state_dict(), '/data/i.anokhin/baseline')\n",
    "# generator.load_state_dict(torch.load('/data/i.anokhin/generator'))\n",
    "# discriminator.load_state_dict(torch.load('/data/i.anokhin/discriminator'))\n",
    "# baseline.load_state_dict(torch.load('/data/i.anokhin/baseline'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator.load_state_dict(torch.load(os.path.join(OUT_DIR, 'model_test2')))\n",
    "history = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "discriminator trainnig\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "114it [00:51,  2.21it/s]"
     ]
    }
   ],
   "source": [
    "generator.train()\n",
    "generator.note_ax.to_train=True\n",
    "generator.note_ax.apply_T=False\n",
    "epoch, history = train_GAN(g_lr = 1e-3,d_lr=1e-3, bl_lr = 1e-3, history = history, criterion=True, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating with no styles:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'time_ax' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-94-5853ae5f731b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# with torch.cuda.device(GPU):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mwrite_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'output/rl_test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/notebook/i.a.anokhin/dl/project_dnn2018/generate.py\u001b[0m in \u001b[0;36mwrite_file\u001b[0;34m(name, results)\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0mTakes\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mall\u001b[0m \u001b[0mnotes\u001b[0m \u001b[0mgenerated\u001b[0m \u001b[0mper\u001b[0m \u001b[0mtrack\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mwrites\u001b[0m \u001b[0mit\u001b[0m \u001b[0mto\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \"\"\"\n\u001b[0;32m--> 176\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/notebook/i.a.anokhin/dl/project_dnn2018/generate.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(models, num_bars, Attention, to_train)\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0mtime_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnote_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack_feature_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime_ax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnote_ax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moverall_information\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0mnote_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m     \u001b[0mtime_ax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_beat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mto_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0mnote_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_T\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'time_ax' is not defined"
     ]
    }
   ],
   "source": [
    "# generator.time_ax.use_beat\n",
    "generator.note_ax.to_train=True\n",
    "generator.note_ax.apply_T=False\n",
    "# import gc\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "generator.eval()\n",
    "# with torch.cuda.device(GPU):\n",
    "write_file('output/rl_test', generate(generator, 32, to_train=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
