{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import torch,torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "from model import Generator, iterate_minibatches, compute_loss, train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "OCTAVE_NUM = 4\n",
    "NOTE_NUM = 12\n",
    "TIME_SCALE = 128\n",
    "\n",
    "\n",
    "class LSTM_discriminator(nn.Module):\n",
    "    def __init__(self,hidden_size = 1000,last_dim = 3):\n",
    "        super(self.__class__, self).__init__()\n",
    "        self.last_dim = last_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.note_lstm = nn.LSTM(input_size = OCTAVE_NUM*last_dim,hidden_size = hidden_size)\n",
    "        self.time_lstm = nn.LSTM(input_size = hidden_size,hidden_size = hidden_size)\n",
    "        self.dense = nn.Linear(hidden_size,1)\n",
    "\n",
    "    def forward(self,data):\n",
    "        # data.size() =  (batch_size, TIME_SCALE, NOTE_NUM*OCTAVE_NUM, last_dim)\n",
    "        # octave_data.size() =  (batch_size, TIME_SCALE, NOTE_NUM,OCTAVE_NUM*last_dim)\n",
    "        batch_size,_,_,_ = data.size()\n",
    "        octave_data = data.view(batch_size,TIME_SCALE,NOTE_NUM,OCTAVE_NUM,self.last_dim)\\\n",
    "                          .view(batch_size,TIME_SCALE,NOTE_NUM,OCTAVE_NUM*self.last_dim)\n",
    "            \n",
    "        # note_lstm_input.size() = (NOTE_NUM, batch_size*TIME_SCALE,OCTAVE_NUM*last_dim)\n",
    "        note_lstm_input = octave_data.view(batch_size*TIME_SCALE,NOTE_NUM,OCTAVE_NUM*self.last_dim)\\\n",
    "                                     .transpose(0,1)\n",
    "        # note_lstm_output.size() = (NOTE_NUM,batch_size*TIME_SCALE,hidden_size)\n",
    "        note_lstm_output, _ = self.note_lstm(note_lstm_input)\n",
    "        # time_lstm_input.size() = (TIME_SCALE,batch_size,hidden_size)\n",
    "        time_lstm_input = note_lstm_output[-1].view(batch_size,TIME_SCALE,self.hidden_size)\\\n",
    "                                          .transpose(0,1)\\\n",
    "        # time_lstm_output.size() = (TIME_SCALE,batch_size,1000)\n",
    "        time_lstm_output, _  = self.time_lstm(time_lstm_input)\n",
    "        # dense_input.size() = (batch_size,1000)\n",
    "        dense_input = time_lstm_output[-1]\n",
    "        # dense_output.size() = (batch_size,1)\n",
    "        dense_output = self.dense(dense_input)\n",
    "        probs = F.sigmoid(dense_output)\n",
    "        return probs\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda:4\" if torch.cuda.is_available() else \"cpu\")\n",
    "# # device = torch.device(\"cpu\")\n",
    "# discriminator = LSTM_discriminator(hidden_size=10).to(device)\n",
    "# np_data = np.random.randn(10,TIME_SCALE,NOTE_NUM*OCTAVE_NUM,3)\n",
    "# data = torch.FloatTensor(np_data).to(device)\n",
    "# discriminator(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_baseline(nn.Module):\n",
    "    def __init__(self,hidden_size = 1000):\n",
    "        super(self.__class__, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.note_lstm = nn.LSTM(input_size = OCTAVE_NUM*3,hidden_size = hidden_size)\n",
    "        self.time_lstm = nn.LSTM(input_size = hidden_size,hidden_size = hidden_size)\n",
    "        self.dense = nn.Linear(hidden_size,1)\n",
    "\n",
    "    def forward(self,data,_):\n",
    "        # data.size() =  (batch_size, TIME_SCALE, NOTE_NUM*OCTAVE_NUM, 3)\n",
    "        # octave_data.size() =  (batch_size, TIME_SCALE, NOTE_NUM,OCTAVE_NUM*3)\n",
    "        batch_size,_,_,_ = data.size()\n",
    "        octave_data = data.view(batch_size,TIME_SCALE,NOTE_NUM,OCTAVE_NUM,3)\\\n",
    "                          .view(batch_size,TIME_SCALE,NOTE_NUM,OCTAVE_NUM*3)\n",
    "            \n",
    "        # note_lstm_input.size() = (NOTE_NUM, batch_size*TIME_SCALE,OCTAVE_NUM*3)\n",
    "        note_lstm_input = octave_data.view(batch_size*TIME_SCALE,NOTE_NUM,OCTAVE_NUM*3)\\\n",
    "                                     .transpose(0,1)\n",
    "        # note_lstm_output.size() = (NOTE_NUM,batch_size*TIME_SCALE,hidden_size)\n",
    "        note_lstm_output, _ = self.note_lstm(note_lstm_input)\n",
    "        # time_lstm_input.size() = (TIME_SCALE,batch_size,hidden_size)\n",
    "        time_lstm_input = note_lstm_output[-1].view(batch_size,TIME_SCALE,self.hidden_size)\\\n",
    "                                          .transpose(0,1)\\\n",
    "        # time_lstm_output.size() = (TIME_SCALE,batch_size,1000)\n",
    "        time_lstm_output, _  = self.time_lstm(time_lstm_input)\n",
    "        # dense_input.size() = (batch_size,1000)\n",
    "        dense_input = time_lstm_output[-1]\n",
    "        # dense_output.size() = (batch_size,1)\n",
    "        dense_output = self.dense(dense_input)\n",
    "        probs = F.sigmoid(dense_output)\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discriminator = LSTM_baseline(hidden_size=1000).to(device)\n",
    "# np_data = np.random.randn(10,TIME_SCALE,NOTE_NUM*OCTAVE_NUM,3)\n",
    "# data = torch.FloatTensor(np_data).to(device)\n",
    "# discriminator(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicGenerator(nn.Module):\n",
    "    def __init__(self,hidden_size = 1000):\n",
    "        super(self.__class__, self).__init__()\n",
    "        self.dense_in = nn.Linear(TIME_SCALE*NOTE_NUM*OCTAVE_NUM*3,hidden_size)\n",
    "        self.dense_out = nn.Linear(hidden_size,TIME_SCALE*NOTE_NUM*OCTAVE_NUM*3)\n",
    "\n",
    "    def forward(self,data,_):\n",
    "        batch_size,_,_,_ = data.size()\n",
    "        data = data.view(batch_size,-1)\n",
    "        hid_data = self.dense_in(data)\n",
    "        out_data = self.dense_out(hid_data)\n",
    "        output = F.sigmoid(out_data.view(batch_size, TIME_SCALE, NOTE_NUM*OCTAVE_NUM, 3))\n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicDiscriminator(nn.Module):\n",
    "    def __init__(self,hidden_size = 1000):\n",
    "        super(self.__class__, self).__init__()\n",
    "        self.dense_in = nn.Linear(TIME_SCALE*NOTE_NUM*OCTAVE_NUM*3,hidden_size)\n",
    "        self.dense_out = nn.Linear(hidden_size,1)\n",
    "\n",
    "    def forward(self,data):\n",
    "        batch_size,_,_,_ = data.size()\n",
    "        data = data.view(batch_size,-1)\n",
    "        hid_data = self.dense_in(data)\n",
    "        out_data = self.dense_out(hid_data)\n",
    "        output = F.sigmoid(out_data)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g_loss(p_fake,sound,in_probs,baseline_pred,eps = 1e-8):\n",
    "#     probs = sound[:,TIME_SCALE//2:,:,:2]*in_probs[:,TIME_SCALE//2:,:,:2]\\\n",
    "#             +(1-sound[:,TIME_SCALE//2:,:,:2])*(1-in_probs[:,TIME_SCALE//2:,:,:2])\n",
    "    probs = sound[:,:,:,:2]*in_probs[:,:,:,:2]\\\n",
    "            +(1-sound[:,:,:,:2])*(1-in_probs[:,:,:,:2])\n",
    "    print(p_fake.mean(),probs.mean())\n",
    "    return -((probs+eps).log().sum(dim =-1).sum(dim =-1).sum(dim =-1)*(p_fake-baseline_pred)).mean()\n",
    "\n",
    "# -(in_probs[:,:,:,2].mean()-1)\n",
    "#     return -(p_fake+eps).log().mean()\n",
    "\n",
    "# loss = g_loss(discriminator(false_example),sound,data_gen,baseline(x_batch,ch_batch))\n",
    "\n",
    "#,sound.data,data_gen)\n",
    "\n",
    "\n",
    "def d_loss(p_fake, p_true,eps = 1e-8):\n",
    "     return -((1-p_fake+eps).log().mean()-(p_true+eps).log().mean())\n",
    "    \n",
    "def bl_loss(bl_pred,real_reward):\n",
    "    return (bl_pred-real_reward).pow(2).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data\n",
    "\n",
    "def sample_sound(data_gen):\n",
    "    size = data_gen.size()\n",
    "    rand = torch.rand(*size).cuda()\n",
    "    sample = (rand<data_gen).type(torch.FloatTensor).cuda()\n",
    "#     sample[:,:,:,2] = data_gen[:,:,:,2]\n",
    "    sample[:,:,:,2] = 1\n",
    "    return sample\n",
    "    \n",
    "\n",
    "def train_GAN(generator,discriminator,baseline,X_loader,Y_loader,num_epochs = 3,g_lr = 0.001, d_lr = 0.001,bl_lr = 0.001):\n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "    g_optimizer = torch.optim.Adam(generator.parameters(),     lr=g_lr)#, betas=(0.5, 0.999))\n",
    "    d_optimizer = torch.optim.Adam(discriminator.parameters(), lr=d_lr, betas=(0.5, 0.999))#, betas=(0.5, 0.999))\n",
    "    bl_optimizer = torch.optim.Adam(baseline.parameters(), lr=bl_lr)\n",
    "    \n",
    "    d_losses = []\n",
    "    g_losses = []\n",
    "    bl_losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        for [x_batch,ch_batch],[y_batch] in zip(X_loader,Y_loader):\n",
    "            x_batch = x_batch.cuda()\n",
    "            ch_batch = ch_batch.cuda()\n",
    "            y_batch = y_batch.cuda()\n",
    "            x_batch[:,:,:,2] = 1\n",
    "            ch_batch[:,:,:,2] = 1\n",
    "            y_batch[:,:,:,2] = 1\n",
    "            # Optimize D\n",
    "\n",
    "            data_gen = generator(x_batch,ch_batch)\n",
    "            sound = sample_sound(data_gen).data\n",
    "            #concat_sound = torch.cat([x_batch[:,1:TIME_SCALE//2+1,:,:],sound[:,TIME_SCALE//2:,:,:]],dim = 1)\n",
    "            loss = d_loss(discriminator(sound), discriminator(y_batch))\n",
    "            d_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "#             print(loss.grad)\n",
    "            d_optimizer.step()\n",
    "            d_losses.append(loss.data.cpu().numpy())\n",
    "        \n",
    "            # Optimize BL\n",
    "            data_gen = generator(x_batch,ch_batch)\n",
    "            sound = sample_sound(data_gen).data\n",
    "            loss = bl_loss(baseline(x_batch,ch_batch),discriminator(sound))\n",
    "            bl_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            bl_optimizer.step()\n",
    "            bl_losses.append(loss.data.cpu().numpy())\n",
    "            \n",
    "            # Optimize G\n",
    "            \n",
    "            data_gen = generator(x_batch,ch_batch)\n",
    "            sound = sample_sound(data_gen).data\n",
    "            #concat_sound = torch.cat([x_batch[:,1:TIME_SCALE//2+1,:,:],sound[:,TIME_SCALE//2:,:,:]],dim = 1)\n",
    "            loss = g_loss(discriminator(sound),sound,data_gen,baseline(x_batch,ch_batch))#,sound.data,data_gen)\n",
    "            g_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "#             print(loss.grad)\n",
    "            g_optimizer.step()\n",
    "            g_losses.append(loss.data.cpu().numpy())\n",
    "    return generator,discriminator,baseline,np.array(g_losses),np.array(d_losses),np.array(bl_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((81, 128, 48, 3),)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dataset import load_all\n",
    "from constants import *\n",
    "\n",
    "styles= [['data/Bach1']]\n",
    "train_data, train_labels = load_all(styles, BATCH_SIZE, TIME_SCALE)\n",
    "N = 2500\n",
    "X_tr = train_data[0][:N]\n",
    "y_tr = train_labels[0][:N]\n",
    "# X_te = train_data[0][N:2*N]\n",
    "train_data[0].shape,\n",
    "# X_te.shape,y_tr.shape,N\n",
    "#y_te = train_labels[0][-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_loader = torch.utils.data.DataLoader(\\\n",
    "            torch.utils.data.TensorDataset(\\\n",
    "            *(torch.FloatTensor(X_tr),\n",
    "            torch.FloatTensor(y_tr))),\\\n",
    "            batch_size=10,shuffle=True)\n",
    "# Y_loader = torch.utils.data.DataLoader(\\\n",
    "#             torch.utils.data.TensorDataset(\\\n",
    "#             torch.FloatTensor(X_te)),\\\n",
    "#             batch_size=10,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data\n",
    "\n",
    "    \n",
    "\n",
    "def train_GAN2(generator,discriminator,baseline,X_loader,num_epochs = 3,g_lr = 0.001, d_lr = 0.001,bl_lr = 0.001):\n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "    g_optimizer = torch.optim.Adam(generator.parameters(),     lr=g_lr)#, betas=(0.5, 0.999))\n",
    "    d_optimizer = torch.optim.Adam(discriminator.parameters(), lr=d_lr,weight_decay = 1)#, betas=(0.5, 0.999))\n",
    "    bl_optimizer = torch.optim.Adam(baseline.parameters(), lr=bl_lr)\n",
    "    \n",
    "    d_losses = []\n",
    "    g_losses = []\n",
    "    bl_losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        for x_batch,ch_batch in X_loader:\n",
    "            x_batch = x_batch.cuda()\n",
    "            ch_batch = ch_batch.cuda()\n",
    "\n",
    "            x_batch[:,:,:,2] = 1\n",
    "            ch_batch[:,:,:,2] = 1\n",
    "\n",
    "            # Optimize D\n",
    "#             print(1)\n",
    "\n",
    "            data_gen = generator(x_batch,ch_batch)\n",
    "            sound = sample_sound(data_gen).data\n",
    "            #concat_sound = torch.cat([x_batch[:,1:TIME_SCALE//2+1,:,:],sound[:,TIME_SCALE//2:,:,:]],dim = 1)\n",
    "            false_example = torch.cat([sound,x_batch],dim = -1)\n",
    "            true_example = torch.cat([x_batch,ch_batch],dim = -1)\n",
    "            loss = d_loss(discriminator(false_example), discriminator(true_example))\n",
    "            d_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "#             print(loss.grad)\n",
    "            d_optimizer.step()\n",
    "            d_losses.append(loss.data.cpu().numpy())\n",
    "        \n",
    "            # Optimize BL\n",
    "            data_gen = generator(x_batch,ch_batch)\n",
    "#             sound = sample_sound(data_gen).data\n",
    "            false_example = torch.cat([sound,x_batch],dim = -1)\n",
    "            loss = bl_loss(baseline(x_batch,ch_batch),discriminator(false_example))\n",
    "            bl_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            bl_optimizer.step()\n",
    "            bl_losses.append(loss.data.cpu().numpy())\n",
    "            \n",
    "            # Optimize G\n",
    "            \n",
    "            data_gen = generator(x_batch,ch_batch)\n",
    "#             sound = sample_sound(data_gen).data\n",
    "            #concat_sound = torch.cat([x_batch[:,1:TIME_SCALE//2+1,:,:],sound[:,TIME_SCALE//2:,:,:]],dim = 1)\n",
    "            false_example = torch.cat([sound,x_batch],dim = -1)\n",
    "            loss = g_loss(discriminator(false_example),sound,data_gen,baseline(x_batch,ch_batch))#,sound.data,data_gen)\n",
    "            g_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "#             print(loss.grad)\n",
    "            g_optimizer.step()\n",
    "            g_losses.append(loss.data.cpu().numpy())\n",
    "    return generator,discriminator,baseline,np.array(g_losses),np.array(d_losses),np.array(bl_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "generator = Generator().cuda()\n",
    "generator.load_state_dict(torch.load(os.path.join(OUT_DIR, 'model_canonical')))\n",
    "discriminator = LSTM_discriminator(hidden_size=100, last_dim=6).cuda()\n",
    "#     # generator = BasicGenerator().cuda()\n",
    "baseline = LSTM_baseline(hidden_size=10).cuda()\n",
    "# discriminator = BasicDiscriminator().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4960, device='cuda:0') tensor(0.8759, device='cuda:0')\n",
      "tensor(0.4908, device='cuda:0') tensor(0.8780, device='cuda:0')\n",
      "tensor(0.4886, device='cuda:0') tensor(0.8793, device='cuda:0')\n",
      "tensor(0.4881, device='cuda:0') tensor(0.8782, device='cuda:0')\n",
      "tensor(0.4883, device='cuda:0') tensor(0.8755, device='cuda:0')\n",
      "tensor(0.4888, device='cuda:0') tensor(0.8716, device='cuda:0')\n",
      "tensor(0.4893, device='cuda:0') tensor(0.8659, device='cuda:0')\n",
      "tensor(0.4896, device='cuda:0') tensor(0.8602, device='cuda:0')\n",
      "tensor(0.4895, device='cuda:0') tensor(0.8590, device='cuda:0')\n",
      "tensor(0.4888, device='cuda:0') tensor(0.8439, device='cuda:0')\n",
      "tensor(0.4877, device='cuda:0') tensor(0.8374, device='cuda:0')\n",
      "tensor(0.4860, device='cuda:0') tensor(0.8261, device='cuda:0')\n",
      "tensor(0.4841, device='cuda:0') tensor(0.8180, device='cuda:0')\n",
      "tensor(0.4817, device='cuda:0') tensor(0.8062, device='cuda:0')\n",
      "tensor(0.4793, device='cuda:0') tensor(0.7955, device='cuda:0')\n",
      "tensor(0.4767, device='cuda:0') tensor(0.7883, device='cuda:0')\n",
      "tensor(0.4741, device='cuda:0') tensor(0.7839, device='cuda:0')\n",
      "tensor(0.4715, device='cuda:0') tensor(0.7764, device='cuda:0')\n",
      "tensor(0.4690, device='cuda:0') tensor(0.7739, device='cuda:0')\n",
      "tensor(0.4666, device='cuda:0') tensor(0.7705, device='cuda:0')\n",
      "tensor(0.4643, device='cuda:0') tensor(0.7691, device='cuda:0')\n",
      "tensor(0.4621, device='cuda:0') tensor(0.7656, device='cuda:0')\n",
      "tensor(0.4600, device='cuda:0') tensor(0.7635, device='cuda:0')\n",
      "tensor(0.4580, device='cuda:0') tensor(0.7599, device='cuda:0')\n",
      "tensor(0.4559, device='cuda:0') tensor(0.7500, device='cuda:0')\n",
      "tensor(0.4539, device='cuda:0') tensor(0.7450, device='cuda:0')\n",
      "tensor(0.4518, device='cuda:0') tensor(0.7328, device='cuda:0')\n",
      "tensor(0.4498, device='cuda:0') tensor(0.7122, device='cuda:0')\n",
      "tensor(0.4476, device='cuda:0') tensor(0.6820, device='cuda:0')\n",
      "tensor(0.4455, device='cuda:0') tensor(0.6622, device='cuda:0')\n",
      "tensor(0.4433, device='cuda:0') tensor(0.6548, device='cuda:0')\n",
      "tensor(0.4411, device='cuda:0') tensor(0.6552, device='cuda:0')\n",
      "tensor(0.4389, device='cuda:0') tensor(0.6749, device='cuda:0')\n",
      "tensor(0.4367, device='cuda:0') tensor(0.6962, device='cuda:0')\n",
      "tensor(0.4345, device='cuda:0') tensor(0.7119, device='cuda:0')\n",
      "tensor(0.4323, device='cuda:0') tensor(0.7219, device='cuda:0')\n",
      "tensor(0.4302, device='cuda:0') tensor(0.7328, device='cuda:0')\n",
      "tensor(0.4280, device='cuda:0') tensor(0.7399, device='cuda:0')\n",
      "tensor(0.4259, device='cuda:0') tensor(0.7457, device='cuda:0')\n",
      "tensor(0.4238, device='cuda:0') tensor(0.7503, device='cuda:0')\n",
      "tensor(0.4217, device='cuda:0') tensor(0.7526, device='cuda:0')\n",
      "tensor(0.4196, device='cuda:0') tensor(0.7535, device='cuda:0')\n",
      "tensor(0.4176, device='cuda:0') tensor(0.7548, device='cuda:0')\n",
      "tensor(0.4155, device='cuda:0') tensor(0.7623, device='cuda:0')\n",
      "tensor(0.4135, device='cuda:0') tensor(0.7615, device='cuda:0')\n",
      "tensor(0.4115, device='cuda:0') tensor(0.7649, device='cuda:0')\n",
      "tensor(0.4095, device='cuda:0') tensor(0.7634, device='cuda:0')\n",
      "tensor(0.4075, device='cuda:0') tensor(0.7589, device='cuda:0')\n",
      "tensor(0.4055, device='cuda:0') tensor(0.7550, device='cuda:0')\n",
      "tensor(0.4036, device='cuda:0') tensor(0.7492, device='cuda:0')\n",
      "tensor(0.4016, device='cuda:0') tensor(0.7419, device='cuda:0')\n",
      "tensor(0.3997, device='cuda:0') tensor(0.7452, device='cuda:0')\n",
      "tensor(0.3978, device='cuda:0') tensor(0.7376, device='cuda:0')\n",
      "tensor(0.3959, device='cuda:0') tensor(0.7312, device='cuda:0')\n",
      "tensor(0.3940, device='cuda:0') tensor(0.7326, device='cuda:0')\n",
      "tensor(0.3921, device='cuda:0') tensor(0.7237, device='cuda:0')\n",
      "tensor(0.3903, device='cuda:0') tensor(0.7153, device='cuda:0')\n",
      "tensor(0.3885, device='cuda:0') tensor(0.7236, device='cuda:0')\n",
      "tensor(0.3867, device='cuda:0') tensor(0.7163, device='cuda:0')\n",
      "tensor(0.3849, device='cuda:0') tensor(0.7072, device='cuda:0')\n",
      "tensor(0.3831, device='cuda:0') tensor(0.7009, device='cuda:0')\n",
      "tensor(0.3813, device='cuda:0') tensor(0.7053, device='cuda:0')\n",
      "tensor(0.3796, device='cuda:0') tensor(0.6999, device='cuda:0')\n",
      "tensor(0.3779, device='cuda:0') tensor(0.7108, device='cuda:0')\n",
      "tensor(0.3762, device='cuda:0') tensor(0.6924, device='cuda:0')\n",
      "tensor(0.3745, device='cuda:0') tensor(0.6644, device='cuda:0')\n",
      "tensor(0.3728, device='cuda:0') tensor(0.6888, device='cuda:0')\n",
      "tensor(0.3712, device='cuda:0') tensor(0.7052, device='cuda:0')\n",
      "tensor(0.3696, device='cuda:0') tensor(0.6993, device='cuda:0')\n",
      "tensor(0.3680, device='cuda:0') tensor(0.6762, device='cuda:0')\n",
      "tensor(0.3664, device='cuda:0') tensor(0.6660, device='cuda:0')\n",
      "tensor(0.3648, device='cuda:0') tensor(0.6661, device='cuda:0')\n",
      "tensor(0.3632, device='cuda:0') tensor(0.6680, device='cuda:0')\n",
      "tensor(0.3617, device='cuda:0') tensor(0.6812, device='cuda:0')\n",
      "tensor(0.3602, device='cuda:0') tensor(0.6869, device='cuda:0')\n",
      "tensor(0.3587, device='cuda:0') tensor(0.6760, device='cuda:0')\n",
      "tensor(0.3572, device='cuda:0') tensor(0.6612, device='cuda:0')\n",
      "tensor(0.3557, device='cuda:0') tensor(0.6526, device='cuda:0')\n",
      "tensor(0.3543, device='cuda:0') tensor(0.6575, device='cuda:0')\n",
      "tensor(0.3528, device='cuda:0') tensor(0.6635, device='cuda:0')\n",
      "tensor(0.3514, device='cuda:0') tensor(0.6727, device='cuda:0')\n",
      "tensor(0.3500, device='cuda:0') tensor(0.6600, device='cuda:0')\n",
      "tensor(0.3486, device='cuda:0') tensor(0.6405, device='cuda:0')\n",
      "tensor(0.3473, device='cuda:0') tensor(0.6286, device='cuda:0')\n",
      "tensor(0.3459, device='cuda:0') tensor(0.6351, device='cuda:0')\n",
      "tensor(0.3446, device='cuda:0') tensor(0.6366, device='cuda:0')\n",
      "tensor(0.3433, device='cuda:0') tensor(0.6387, device='cuda:0')\n",
      "tensor(0.3420, device='cuda:0') tensor(0.6427, device='cuda:0')\n",
      "tensor(0.3407, device='cuda:0') tensor(0.6345, device='cuda:0')\n",
      "tensor(0.3394, device='cuda:0') tensor(0.6001, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "generator,discriminator,baseline,g_losses,d_losses,bl_losses =\\\n",
    "                    train_GAN2(generator,discriminator,baseline,\\\n",
    "                            X_loader,num_epochs = 10, g_lr = 1*1e-4,d_lr=1*1e-2, bl_lr = 1*1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'g_losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-dab592e2e3ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg_losses\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Generator loss\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_losses\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Discriminator loss\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'g_losses' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(g_losses,label = \"Generator loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.plot(d_losses,label = \"Discriminator loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.plot(bl_losses,label = \"Baseline loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  184.20493  ,   142.3104   ,   120.018654 ,   114.6346   ,\n",
       "         108.85241  ,   109.8381   ,   107.95536  ,   108.08594  ,\n",
       "         101.41591  ,   106.14627  ,    99.209076 ,    97.43999  ,\n",
       "          87.86245  ,    80.27351  ,    69.82189  ,    57.58156  ,\n",
       "          42.455685 ,    31.59626  ,    20.003407 ,     6.9552355,\n",
       "          -4.4558682,   -14.864026 ,   -28.5371   ,   -41.188553 ,\n",
       "         -53.101532 ,   -65.043465 ,   -80.60386  ,   -98.00824  ,\n",
       "        -122.73334  ,  -147.31163  ,  -166.10214  ,  -180.55585  ,\n",
       "        -188.92476  ,  -193.17213  ,  -194.29262  ,  -203.89731  ,\n",
       "        -208.39008  ,  -220.39517  ,  -227.20186  ,  -235.7833   ,\n",
       "        -251.84921  ,  -267.08987  ,  -280.80362  ,  -276.62003  ,\n",
       "        -298.37134  ,  -302.02597  ,  -317.96686  ,  -343.77298  ,\n",
       "        -366.0577   ,  -387.9134   ,  -424.59454  ,  -432.597    ,\n",
       "        -473.75946  ,  -509.76776  ,  -518.22876  ,  -567.5733   ,\n",
       "        -604.7329   ,  -605.6982   ,  -658.8878   ,  -712.11584  ,\n",
       "        -757.7901   ,  -774.26764  ,  -829.81793  ,  -805.79407  ,\n",
       "        -915.9473   , -1036.2957   , -1005.0709   ,  -962.49634  ,\n",
       "       -1030.149    , -1167.9769   , -1248.8309   , -1254.7289   ,\n",
       "       -1315.7952   , -1311.1542   , -1314.8302   , -1430.6171   ,\n",
       "       -1531.5647   , -1632.7678   , -1663.2462   , -1675.8237   ,\n",
       "       -1690.6113   , -1821.0525   , -1977.9446   , -2119.5012   ,\n",
       "       -2160.6873   , -2190.9712   , -2257.5806   , -2300.1672   ,\n",
       "       -2398.225    , -2740.6077   ], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/256 [00:00<00:39,  6.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating with no styles:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 256/256 [00:21<00:00, 11.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing file out/samples/output/rl_test_0.mid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from generate import write_file, generate\n",
    "# import gc\n",
    "# torch.cuda.empty_cache()\n",
    "# gc.collect() \n",
    "\n",
    "# with torch.cuda.device(GPU):\n",
    "write_file('output/rl_test', generate(generator, 16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
